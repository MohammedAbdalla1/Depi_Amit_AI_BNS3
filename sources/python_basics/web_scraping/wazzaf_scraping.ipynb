{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be0d9110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2025.7.14-py3-none-any.whl.metadata (2.4 kB)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl (105 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading certifi-2025.7.14-py3-none-any.whl (162 kB)\n",
      "Installing collected packages: urllib3, idna, charset_normalizer, certifi, requests\n",
      "\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   -------- ------------------------------- 1/5 [idna]\n",
      "   -------- ------------------------------- 1/5 [idna]\n",
      "   -------- ------------------------------- 1/5 [idna]\n",
      "   ---------------- ----------------------- 2/5 [charset_normalizer]\n",
      "   ---------------- ----------------------- 2/5 [charset_normalizer]\n",
      "   ---------------- ----------------------- 2/5 [charset_normalizer]\n",
      "   ---------------- ----------------------- 2/5 [charset_normalizer]\n",
      "   ------------------------ --------------- 3/5 [certifi]\n",
      "   -------------------------------- ------- 4/5 [requests]\n",
      "   -------------------------------- ------- 4/5 [requests]\n",
      "   -------------------------------- ------- 4/5 [requests]\n",
      "   ---------------------------------------- 5/5 [requests]\n",
      "\n",
      "Successfully installed certifi-2025.7.14 charset_normalizer-3.4.2 idna-3.10 requests-2.32.4 urllib3-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6496bdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b637d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r\"https://wuzzuf.net/search/jobs?a=spbg&q=machine%20learning\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c161c0ec",
   "metadata": {},
   "source": [
    "Getting the jobs names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45cd409d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Engineer - Travel Tech\n",
      "Data Scientist\n",
      "Data Scientist\n",
      "Graduate Internship: Enterprise Software Engineering\n",
      "AI Engineer\n",
      "Machine Learning Engineer\n",
      "Machine Learning Engineer\n",
      "AI Engineer (Computer Vision, Speech, NLP & Generative AI)\n",
      "Learning & Development Specialist\n",
      "Learning & Development Manager - LXR Eastern Mangroves Abu Dhabi (Pre-Opening)\n",
      "3D Printer Maintenance Technician\n",
      "AI Engineer (Custom AI Solutions & GPT Tools) – On-Site\n",
      "Senior Data Analyst\n",
      "AI Engineer - Client Engineering KSA\n",
      "MLOps Engineer\n"
     ]
    }
   ],
   "source": [
    "jobs_names = soup.find_all('h2', class_ = \"css-m604qf\")\n",
    "for job in jobs_names:\n",
    "    print(job.text.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3aaa8d",
   "metadata": {},
   "source": [
    "Getting companies names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1de2b5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rahala\n",
      "Egabi FSI\n",
      "Lumin\n",
      "eT3  Tomorrow Information Technology\n",
      "ProVision Group\n",
      "EPAM Systems\n",
      "Mozn\n",
      "Madar Soft\n",
      "Sungrow MENA & Central Asia\n",
      "Hilton\n",
      "Gazelle 3D Tech\n",
      "Stellar Charter Nexus\n",
      "Xceed Contact Center\n",
      "IBM\n",
      "Confidential\n"
     ]
    }
   ],
   "source": [
    "comp_names = soup.find_all('a',class_ = \"css-17s97q8\")\n",
    "for comp in comp_names:\n",
    "    print(comp.text.replace('-','').strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5d6a13",
   "metadata": {},
   "source": [
    "Getting addresses of the company:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "581fd720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mohandessin, Giza, Egypt \n",
      "Sheraton, Cairo, Egypt \n",
      "Sheikh Zayed, Giza, Egypt \n",
      "New Cairo, Cairo, Egypt \n",
      "Maadi, Cairo, Egypt \n",
      "Dubai, United Arab Emirates \n",
      "Riyadh, Saudi Arabia \n",
      "Fleming, Alexandria, Egypt \n",
      "Dubai, United Arab Emirates \n",
      "Dubai, United Arab Emirates \n",
      "6th of October, Giza, Egypt \n",
      "New Cairo, Cairo, Egypt \n",
      "Cairo, Egypt \n",
      "Riyadh, Saudi Arabia \n",
      "Cairo, Egypt \n"
     ]
    }
   ],
   "source": [
    "comp_addresses = soup.find_all('span', class_ = 'css-5wys0k')\n",
    "for address in comp_addresses:\n",
    "    print(address.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7e1559",
   "metadata": {},
   "source": [
    "Getting posting time of the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d3369d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 days ago\n",
      "7 days ago\n",
      "7 days ago\n",
      "27 minutes ago\n",
      "4 days ago\n",
      "4 days ago\n",
      "7 days ago\n"
     ]
    }
   ],
   "source": [
    "posting_time = soup.find_all('div', class_ = \"css-4c4ojb\")\n",
    "for p_time in posting_time:\n",
    "    print(p_time.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cc38b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " · AI\n",
      " · Computer Science\n",
      " · Data Science\n",
      " · Engineering\n",
      " · Information Technology (IT)\n",
      "Machine Learning\n",
      " · Python\n",
      " · Computer Science\n",
      " · Data Mining\n",
      " · Data Science\n",
      " · data scientist\n",
      " · Hadoop\n",
      "Machine Learning\n",
      " · Python\n",
      " · SQL\n",
      " · Computer Science\n",
      " · Analysis\n",
      " · Mathematics\n",
      " · Data Analysis\n",
      " · Hadoop\n",
      " · Data Science\n",
      " · Information Technology (IT)\n",
      " · Computer Science\n",
      " · Software Development\n",
      " · Software Engineering\n",
      " · Computer Engineering\n",
      " · Back-End Development\n",
      " · Front-End Development\n",
      " · front-end web development\n",
      " · Software Testing\n",
      " · Data Engineering\n",
      " · AI\n",
      " · Computer Vision\n",
      " · Data Science\n",
      "Machine Learning\n",
      "Machine Learning\n",
      " · Computer Science\n",
      " · Information Technology (IT)\n",
      " · Python\n",
      " · Software Engineering\n",
      " · Computer Engineering\n",
      " · Engineering\n",
      "Machine Learning\n",
      " · Computer Science\n",
      " · Information Technology (IT)\n",
      " · Python\n",
      " · Software Engineering\n",
      " · Computer Engineering\n",
      " · Engineering\n",
      " · computer\n",
      " · AI\n",
      " · NLP\n",
      " · Engineering\n",
      " · Information Technology (IT)\n",
      " · CUDA\n",
      " · Linux\n",
      " · Computer Science\n",
      " · Education\n",
      "Learning\n",
      " · Training\n",
      " · Information Technology (IT)\n",
      " · Computer Science\n",
      " · Teaching\n",
      "E-Learning\n",
      " · Education\n",
      "Learning\n",
      " · Training\n",
      " · Information Technology (IT)\n",
      " · Computer Science\n",
      " · Teaching\n",
      "E-Learning\n",
      " · 3D Printing\n",
      "Machines\n",
      " · Maintenance\n",
      " · AI\n",
      " · APIs\n",
      " · Computer Science\n",
      " · Development\n",
      " · Engineering\n",
      " · Frameworks\n",
      " · Information Technology (IT)\n",
      " · ETL\n",
      " · Data Analysis\n",
      " · Communication skills\n",
      " · Python\n",
      " · SQL\n",
      " · Business Intelligence\n",
      " · PowerBI\n",
      " · Pandas\n",
      " · AI\n",
      " · Information Technology (IT)\n",
      " · Computer Science\n",
      " · Engineering\n",
      " · Python\n",
      " · Software Engineering\n",
      " · NLP\n",
      " · Cloudera\n",
      " · MLOps\n",
      " · MLflow\n",
      " · Hadoop\n"
     ]
    }
   ],
   "source": [
    "required_skills = soup.find_all('a',class_ = \"css-5x9pm1\")\n",
    "for skills in required_skills:\n",
    "    print(skills.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

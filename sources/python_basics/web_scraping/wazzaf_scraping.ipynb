{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be0d9110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\maydoum\\onedrive\\desktop\\depi\\projects\\depi_amit_ai_bns3\\.venv\\lib\\site-packages (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\maydoum\\onedrive\\desktop\\depi\\projects\\depi_amit_ai_bns3\\.venv\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\maydoum\\onedrive\\desktop\\depi\\projects\\depi_amit_ai_bns3\\.venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\maydoum\\onedrive\\desktop\\depi\\projects\\depi_amit_ai_bns3\\.venv\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maydoum\\onedrive\\desktop\\depi\\projects\\depi_amit_ai_bns3\\.venv\\lib\\site-packages (from requests) (2025.7.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6496bdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b637d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r\"https://wuzzuf.net/search/jobs?a=spbg&q=machine%20learning\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c161c0ec",
   "metadata": {},
   "source": [
    "Getting the jobs names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45cd409d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Engineer - Travel Tech\n",
      "Data Scientist\n",
      "Data Scientist\n",
      "Graduate Internship: Enterprise Software Engineering\n",
      "AI Engineer\n",
      "Machine Learning Engineer\n",
      "Machine Learning Engineer\n",
      "AI Engineer (Computer Vision, Speech, NLP & Generative AI)\n",
      "Learning & Development Specialist\n",
      "Learning & Development Manager - LXR Eastern Mangroves Abu Dhabi (Pre-Opening)\n",
      "3D Printer Maintenance Technician\n",
      "Senior Data Analyst\n",
      "AI Engineer - Client Engineering KSA\n",
      "MLOps Engineer\n",
      "Software Engineer (AI & Data Analytics)\n"
     ]
    }
   ],
   "source": [
    "jobs_names = soup.find_all('h2', class_ = \"css-m604qf\")\n",
    "for job in jobs_names:\n",
    "    print(job.text.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3aaa8d",
   "metadata": {},
   "source": [
    "Getting companies names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1de2b5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rahala\n",
      "Lumin\n",
      "Egabi FSI\n",
      "eT3  Tomorrow Information Technology\n",
      "ProVision Group\n",
      "EPAM Systems\n",
      "Mozn\n",
      "Madar Soft\n",
      "Sungrow MENA & Central Asia\n",
      "Hilton\n",
      "Gazelle 3D Tech\n",
      "Xceed Contact Center\n",
      "IBM\n",
      "Confidential\n",
      "AlZamel for consultation\n"
     ]
    }
   ],
   "source": [
    "comp_names = soup.find_all('a',class_ = \"css-17s97q8\")\n",
    "for comp in comp_names:\n",
    "    print(comp.text.replace('-','').strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5d6a13",
   "metadata": {},
   "source": [
    "Getting addresses of the company:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "581fd720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mohandessin, Giza, Egypt \n",
      "Sheikh Zayed, Giza, Egypt \n",
      "Sheraton, Cairo, Egypt \n",
      "New Cairo, Cairo, Egypt \n",
      "Maadi, Cairo, Egypt \n",
      "Dubai, United Arab Emirates \n",
      "Riyadh, Saudi Arabia \n",
      "Fleming, Alexandria, Egypt \n",
      "Dubai, United Arab Emirates \n",
      "Dubai, United Arab Emirates \n",
      "6th of October, Giza, Egypt \n",
      "Cairo, Egypt \n",
      "Riyadh, Saudi Arabia \n",
      "Cairo, Egypt \n",
      "Maadi, Cairo, Egypt \n"
     ]
    }
   ],
   "source": [
    "comp_addresses = soup.find_all('span', class_ = 'css-5wys0k')\n",
    "for address in comp_addresses:\n",
    "    print(address.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7e1559",
   "metadata": {},
   "source": [
    "Getting posting time of the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d3369d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 days ago\n",
      "2 days ago\n",
      "6 days ago\n",
      "6 days ago\n"
     ]
    }
   ],
   "source": [
    "posting_time = soup.find_all('div', class_ = \"css-4c4ojb\")\n",
    "for p_time in posting_time:\n",
    "    print(p_time.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cc38b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " · AI\n",
      " · Computer Science\n",
      " · Data Science\n",
      " · Engineering\n",
      " · Information Technology (IT)\n",
      "Machine Learning\n",
      " · Python\n",
      "---------------\n",
      " · Computer Science\n",
      " · Analysis\n",
      " · Mathematics\n",
      " · Data Analysis\n",
      " · Hadoop\n",
      " · Data Science\n",
      " · Information Technology (IT)\n",
      "---------------\n",
      " · Computer Science\n",
      " · Data Mining\n",
      " · Data Science\n",
      " · data scientist\n",
      " · Hadoop\n",
      "Machine Learning\n",
      " · Python\n",
      " · SQL\n",
      "---------------\n",
      " · Computer Science\n",
      " · Software Development\n",
      " · Software Engineering\n",
      " · Computer Engineering\n",
      " · Back-End Development\n",
      " · Front-End Development\n",
      " · front-end web development\n",
      " · Software Testing\n",
      " · Data Engineering\n",
      "---------------\n",
      " · AI\n",
      " · Computer Vision\n",
      " · Data Science\n",
      "Machine Learning\n",
      "---------------\n",
      "Machine Learning\n",
      " · Computer Science\n",
      " · Information Technology (IT)\n",
      " · Python\n",
      " · Software Engineering\n",
      " · Computer Engineering\n",
      " · Engineering\n",
      "---------------\n",
      "Machine Learning\n",
      " · Computer Science\n",
      " · Information Technology (IT)\n",
      " · Python\n",
      " · Software Engineering\n",
      " · Computer Engineering\n",
      " · Engineering\n",
      "---------------\n",
      " · computer\n",
      " · AI\n",
      " · NLP\n",
      " · Engineering\n",
      " · Information Technology (IT)\n",
      " · CUDA\n",
      " · Linux\n",
      " · Computer Science\n",
      "---------------\n",
      " · Education\n",
      "Learning\n",
      " · Training\n",
      " · Information Technology (IT)\n",
      " · Computer Science\n",
      " · Teaching\n",
      "E-Learning\n",
      "---------------\n",
      " · Education\n",
      "Learning\n",
      " · Training\n",
      " · Information Technology (IT)\n",
      " · Computer Science\n",
      " · Teaching\n",
      "E-Learning\n",
      "---------------\n",
      " · 3D Printing\n",
      "Machines\n",
      " · Maintenance\n",
      "---------------\n",
      " · ETL\n",
      " · Data Analysis\n",
      " · Communication skills\n",
      " · Python\n",
      " · SQL\n",
      " · Business Intelligence\n",
      " · PowerBI\n",
      " · Pandas\n",
      "---------------\n",
      " · AI\n",
      " · Information Technology (IT)\n",
      " · Computer Science\n",
      " · Engineering\n",
      " · Python\n",
      " · Software Engineering\n",
      " · NLP\n",
      "---------------\n",
      " · Cloudera\n",
      " · MLOps\n",
      " · MLflow\n",
      " · Hadoop\n",
      "---------------\n",
      " · Analysis\n",
      " · Information Technology (IT)\n",
      " · Cloud\n",
      " · Software Engineering\n",
      " · Software Development\n",
      " · Java\n",
      " · Programming\n",
      " · AWS\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "job_cards = soup.find_all('div', class_ = \"css-1gatmva e1v1l3u10\")\n",
    "\n",
    "for job in job_cards:\n",
    "    required_skills = job.find_all('a',class_ = \"css-5x9pm1\")\n",
    "    for skills in required_skills:\n",
    "        print(skills.text)\n",
    "    print(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b283bb26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
